{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b5fad9",
   "metadata": {},
   "source": [
    " # import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c91c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    pairwise_distances,\n",
    "    mutual_info_score,\n",
    ")\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from tslearn.metrics import cdist_dtw\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc76f7",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27afd8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def z_normalize(T):\n",
    "    return (T - np.mean(T))/np.std(T)\n",
    "\n",
    "def replace_isolated_missed_value(T, isolated_missing_index):\n",
    "    T[isolated_missing_index] = 0.5 * (\n",
    "        T[isolated_missing_index - 1] + T[isolated_missing_index+1]\n",
    "    )\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9949014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_med_multi_centroid_Grouped_HighLevel_Featue_DTWenhanced(dict_of_data, k=30, n_itr=5, rs=0, \n",
    "                                                    dict_of_data_actual=None, Grouped_Features=None, keys2remove=[], w=1,\n",
    "                                                    DTWmat_GroupedFeature_input=[], OneDay_flag=False, weighted=False, \n",
    "                                                    only_last_itr=True,\n",
    "                                                    weighted_last_iteration = False):\n",
    "    for key in keys2remove:\n",
    "        del dict_of_data[key]\n",
    "        del dict_of_data_actual[key]\n",
    "\n",
    "\n",
    "    dict_of_data_keys = list(dict_of_data.keys())\n",
    "    nkey = len(dict_of_data_keys)\n",
    "\n",
    "    dict_of_data_keys_idx = {}\n",
    "    for idx_mykey, my_key in enumerate(dict_of_data_keys):\n",
    "        dict_of_data_keys_idx[my_key] = idx_mykey\n",
    "\n",
    "  \n",
    "    N, H = np.shape(dict_of_data[dict_of_data_keys[0]])\n",
    "  \n",
    "    # getting distance matices of different groups\n",
    "    if len(DTWmat_GroupedFeature_input)==0:\n",
    "        DTWdistmat_of_Grouped_Features = []\n",
    "        for j_item, item in enumerate(Grouped_Features):\n",
    "            third_dim = len(item)\n",
    "            data = np.zeros((N,H,third_dim))\n",
    "            dmat = 0\n",
    "            for idx_data_key, data_key in enumerate(item):\n",
    "                dmat += pairwise_distances(dict_of_data[data_key])\n",
    "                data[:,:,idx_data_key] = dict_of_data[data_key]\n",
    "\n",
    "            distmat_temp = cdist_dtw(data, global_constraint=\"sakoe_chiba\", sakoe_chiba_radius=w) #dmat\n",
    "            DTWdistmat_of_Grouped_Features.append(distmat_temp)\n",
    "    else:\n",
    "        DTWdistmat_of_Grouped_Features = DTWmat_GroupedFeature_input[:]\n",
    "\n",
    "\n",
    "    # To investigate the impact of weighted approach, default: False\n",
    "    if weighted:\n",
    "        normalizer = [len(x) for x in Grouped_Features]\n",
    "    else:\n",
    "        normalizer = np.ones(len(DTWdistmat_of_Grouped_Features))\n",
    "\n",
    "    if weighted_last_iteration:\n",
    "        normalizer_inner = [len(x) for x in Grouped_Features]\n",
    "    else:\n",
    "        normalizer_inner = np.ones(len(DTWdistmat_of_Grouped_Features))\n",
    "\n",
    "\n",
    "    print(f'normalizer: {normalizer}')\n",
    "\n",
    "    ts_all = np.zeros((N,0))\n",
    "    for ikey,key in enumerate(dict_of_data_keys):\n",
    "        ts_all = np.concatenate((ts_all,dict_of_data[key]),axis=1)\n",
    "\n",
    "    distmat_all = sum(np.square(x) for x in DTWdistmat_of_Grouped_Features)\n",
    "\n",
    "    distmat_dict = {}\n",
    "    for key in dict_of_data_keys:\n",
    "        distmat_dict[key] = pairwise_distances(dict_of_data[key])\n",
    "\n",
    "    km = KMedoids(n_clusters=k, init=\"k-medoids++\")\n",
    "    centroid_init = km._kpp_init(distmat_all, n_clusters=k, random_state_=check_random_state(rs))\n",
    "    labels = np.argmin(distmat_all[centroid_init], axis=0)\n",
    "\n",
    "    cmm_old = np.zeros((N,N), dtype='int')\n",
    "    np.fill_diagonal(cmm_old, 1)\n",
    "\n",
    "    labels_diff_vec = np.zeros(n_itr)\n",
    "    inertia_tot_vec = np.zeros(n_itr)\n",
    "    inertia_tot_per_item_vec = np.zeros((len(Grouped_Features), n_itr))\n",
    "\n",
    "\n",
    "    intertia_tot_old = 0\n",
    "    for itr in range(n_itr):\n",
    "        center_id = np.empty((k,nkey), dtype='int')\n",
    "        center_val = np.empty(shape=(k,H))\n",
    "    \n",
    "        inertia_tot = 0\n",
    "        inertia_tot_per_item = np.zeros(len(Grouped_Features))\n",
    "    \n",
    "        for ik in range(k):\n",
    "            days = np.ix_(labels==ik)[0]\n",
    "            if len(days)==1:\n",
    "                center_id[ik,:] = days[0]\n",
    "                inertia_tot += 0\n",
    "\n",
    "            else:\n",
    "                for j_item, item in enumerate(Grouped_Features):\n",
    "                    distmat = DTWdistmat_of_Grouped_Features[j_item]\n",
    "                    distmat_sliced = distmat[np.ix_(days,days)]\n",
    "\n",
    "                    # considering w=0 is just for the sake of consistency. (when w=0, it is basically ED!)\n",
    "                    if w==0:\n",
    "                        distmat_sliced_sq = np.square(distmat_sliced) #distmat_sliced \n",
    "                    else:\n",
    "                        distmat_sliced_sq = np.square(distmat_sliced) #distmat_sliced np.square(distmat_sliced)\n",
    "\n",
    "\n",
    "                    dist2others = np.sum(distmat_sliced_sq,axis=1)\n",
    "                    sel_day =  days[np.argmin(dist2others)]\n",
    "                    \n",
    "                    # construct/update hybrid centroids\n",
    "                    for key in item:\n",
    "                        center_id[ik,dict_of_data_keys_idx[key]] = sel_day\n",
    "\n",
    "\n",
    "\n",
    "                    inertia_tot_per_item[j_item] += np.min(np.sum(distmat_sliced_sq,axis=1))\n",
    "\n",
    "                    inertia_tot += np.min(np.sum(distmat_sliced_sq,axis=1))\n",
    "        \n",
    "\n",
    "        inertia_tot_diff = inertia_tot - intertia_tot_old\n",
    "        if abs(inertia_tot_diff)<0.001: # break update if the improvement is less than 0.001\n",
    "            break\n",
    "\n",
    "        intertia_tot_old = inertia_tot\n",
    "        inertia_tot_per_item_vec[:,itr] = inertia_tot_per_item\n",
    "\n",
    "\n",
    "        center_val = np.zeros((k,0))\n",
    "        for ikey,key in enumerate(dict_of_data_keys):\n",
    "            centers_id_of_key = center_id[:,ikey]    \n",
    "            center_val = np.concatenate((center_val, dict_of_data[key][centers_id_of_key]), axis=1)\n",
    "\n",
    "        OneDay_center_id = np.empty(k, dtype='int')\n",
    "        if centroid_condition(n_itr, itr, only_last_itr, OneDay_flag):\n",
    "            for ik in range(k):\n",
    "                days = np.ix_(labels==ik)[0]\n",
    "                if len(days)==1:\n",
    "                    OneDay_center_id[ik] = days[0]\n",
    "                else:\n",
    "                    dvec = np.zeros(len(days))\n",
    "                    for idx_d, d in enumerate(days):\n",
    "                        dvec[idx_d] = dist_to_center_w(\n",
    "                            dict_of_data,\n",
    "                            dict_of_data_keys,\n",
    "                            Grouped_Features,\n",
    "                            DTWdistmat_of_Grouped_Features, \n",
    "                            center_id[ik,:],\n",
    "                            d,\n",
    "                            dict_of_data_keys_idx,\n",
    "                            normalizer_inner\n",
    "                        )\n",
    "\n",
    "\n",
    "                OneDay_center_id[ik] = days[np.argmin(dvec)]\n",
    "\n",
    "            D = pairwise_distances(ts_all , ts_all[OneDay_center_id]) \n",
    "\n",
    "        else:\n",
    "            D = np.zeros((N,k))\n",
    "            for ik in range(k):\n",
    "                for j_item, item in enumerate(Grouped_Features):\n",
    "                    distmat = DTWdistmat_of_Grouped_Features[j_item]\n",
    "\n",
    "                    if w==0:\n",
    "                        distmat_sq = np.square(distmat) #distmat\n",
    "                    else:\n",
    "                        distmat_sq = np.square(distmat) #distmat\n",
    "                    sel_day = int(center_id[ik,dict_of_data_keys_idx[item[0]]])\n",
    "                    D[:,ik] += distmat_sq[:,sel_day]/normalizer[j_item]\n",
    "\n",
    "        labels = np.argmin(D,axis=1)\n",
    "        cmm_new = cmm_creator(labels)\n",
    "        labels_diff_vec[itr] = np.sum(np.abs(cmm_new - cmm_old))\n",
    "\n",
    "        cmm_old = cmm_new\n",
    "        inertia_tot_vec[itr] = inertia_tot\n",
    "    \n",
    "    dict4dc = dict_of_data_actual\n",
    "    \n",
    "    # calc errors for the sake of on-line investigation \n",
    "    err_dc_dict={}\n",
    "    for ikey, key in enumerate(dict_of_data_keys):\n",
    "        if OneDay_flag:\n",
    "            my_center_id = OneDay_center_id[:]\n",
    "        else:\n",
    "            my_center_id = center_id[:,ikey]\n",
    "\n",
    "        data_replica = reconstruct_data(dict4dc[key],labels, my_center_id)\n",
    "        err=err_of_dc_grouply(dict4dc[key],data_replica, labels, my_center_id, day_wise=False, normalized=True, use_normalizd_form=False)\n",
    "\n",
    "        err_dc_dict[key] = err\n",
    "    \n",
    "    # force to avoid hybrid (default: False).\n",
    "    # this is for the sake of investigation\n",
    "    if OneDay_flag:\n",
    "        nr,nc = center_id.shape\n",
    "        for c in range(nc):\n",
    "            center_id[:,c] = OneDay_center_id\n",
    "    \n",
    "    # To handle cases when it stuck into a local optima loop\n",
    "    if np.abs(inertia_tot_diff) > 0.001:\n",
    "        inertia_tot_vec[-1] = 40000\n",
    "\n",
    "\n",
    "    return ts_all, labels, center_id, dict_of_data_keys, inertia_tot_vec[-1], err_dc_dict, DTWdistmat_of_Grouped_Features, OneDay_center_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ebcba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical clustering with min-max linkage \n",
    "# this is the second stage of KmHC\n",
    "# when w=0, it is basically ED!\n",
    "def HC_MinMax_klst(data, k_arr, n_init=None, w=0, flag_radius_avg=False,flag_distmat=False,given_distmat=None):\n",
    "    \n",
    "    shape_info = list(np.shape(data))\n",
    "    N = shape_info[0]\n",
    "    H = shape_info[1]\n",
    "    D = 0 if len(shape_info)<3 else shape_info[-1]\n",
    "    \n",
    "    if not flag_distmat:\n",
    "        if w>0:\n",
    "            distmatall = cdist_dtw(data,global_constraint='sakoe_chiba',sakoe_chiba_radius=w)\n",
    "        else:\n",
    "            distmatall = pairwise_distances(data)\n",
    "    else:\n",
    "        distmatall = given_distmat\n",
    "        \n",
    "    distmat_group = copy.deepcopy(distmatall)\n",
    "    \n",
    "    cmm = np.eye(N)\n",
    "    groups = find_groups_from_cmm(cmm)\n",
    "    ng = len(groups)\n",
    "\n",
    "    nk = len(k_arr)    \n",
    "    labels_mat = np.zeros((nk,N))\n",
    "      \n",
    "    cnt=0\n",
    "    k_min = np.min(k_arr)\n",
    "    while ng>k_min:\n",
    "        groups_keys = list(groups.keys())\n",
    "         \n",
    "        np.fill_diagonal(distmat_group,float('inf'))\n",
    "        closest_dist = np.min(distmat_group,axis=1)\n",
    "        closest_dist_ind = np.argmin(distmat_group,axis=1)      \n",
    "        ind0 = np.argmin(closest_dist)\n",
    "        ind1 = closest_dist_ind[ind0]\n",
    "            \n",
    "        key0 = groups_keys[ind0]\n",
    "        key1 = groups_keys[ind1]\n",
    "        \n",
    "        days0 = np.array(groups[key0])\n",
    "        days1 = np.array(groups[key1])\n",
    "        \n",
    "        days_joined = np.reshape(np.row_stack((days0.reshape(-1,1),days1.reshape(-1,1))),newshape=(-1,))\n",
    "        days_joined = [x for x in days_joined]\n",
    "        \n",
    "        del groups[key0]\n",
    "        del groups[key1]\n",
    "        \n",
    "        new_key = 1000 + cnt\n",
    "        groups[new_key] = days_joined\n",
    "        groups_keys = list(groups.keys())\n",
    "        \n",
    "        distmat_group = np.delete(distmat_group,[ind0,ind1],axis=0)\n",
    "        distmat_group = np.delete(distmat_group,[ind0,ind1],axis=1)\n",
    "        \n",
    "        distmat_group_new = np.zeros((ng-1,ng-1))\n",
    "        distmat_group_new[:-1,:-1] = distmat_group\n",
    "        \n",
    "        for i in range(ng-2):\n",
    "            days0 = np.array(groups[groups_keys[i]])\n",
    "            days1 = np.array(groups[new_key])\n",
    "            days_joined = np.reshape(np.row_stack((days0.reshape(-1,1),days1.reshape(-1,1))),newshape=(-1,))\n",
    "            days_joined = [x for x in days_joined]\n",
    "            \n",
    "            if not flag_radius_avg:\n",
    "                val = np.min(np.max(distmatall[np.ix_(days_joined,days_joined)],axis=1))\n",
    "                         \n",
    "            distmat_group_new[-1,i] = val\n",
    "            distmat_group_new[i,-1] = val\n",
    "        \n",
    "        distmat_group_new[-1,-1] = float('inf')\n",
    "        distmat_group = distmat_group_new\n",
    "        \n",
    "        ng = len(distmat_group)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if ng in k_arr:\n",
    "            index_of_ng = np.ix_(k_arr==ng)[0][0]\n",
    "            for i,key in enumerate(groups):\n",
    "                for d in groups[key]:\n",
    "                    labels_mat[index_of_ng,d] = i\n",
    "    \n",
    "        \n",
    "        cnt += 1\n",
    "        \n",
    "    return labels_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ee3c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some func to measure edc:\n",
    "\n",
    "# for on-line investigation and analysis (can be skipped)\n",
    "def err_of_dc_grouply(data,data_replica, label, center_id, day_wise=False, normalized=True, use_normalizd_form=False):\n",
    "    \n",
    "    kl=len(set(label))\n",
    "    err_vec=np.zeros(kl)\n",
    "    err_vec_daily=np.zeros(np.shape(data)[0])\n",
    "    t=8760\n",
    "    \n",
    "    if use_normalizd_form==False:\n",
    "        size_vec=np.zeros(kl)\n",
    "      \n",
    "        if day_wise is not None:\n",
    "            for ik in range(kl):\n",
    "                days=np.ix_(label==ik)[0]\n",
    "                if len(days)==1:\n",
    "                    err_vec[ik]=0\n",
    "                    err_vec_daily[days[0]]=0\n",
    "                else:\n",
    "                    if day_wise==False:\n",
    "                        arr1 = np.sort(np.reshape(data[np.ix_(days,)],newshape=(-1,)))[::-1]\n",
    "                        arr2 = np.sort(np.reshape(data_replica[np.ix_(days,)],newshape=(-1,)))[::-1]\n",
    "                        err_vec[ik] = err_func(arr1,arr2)\n",
    "\n",
    "                    elif day_wise==True:\n",
    "                        for d in days:\n",
    "                            arr1 = np.sort(np.reshape(data[d,],newshape=(-1,)))[::-1]\n",
    "                            arr2 = np.sort(np.reshape(data_replica[d,],newshape=(-1,)))[::-1]\n",
    "\n",
    "                            err_vec_daily[d] =  err_func(arr1,arr2)\n",
    "\n",
    "            if day_wise==False:\n",
    "                err=err_vec\n",
    "            elif day_wise==True:\n",
    "                err=err_vec_daily\n",
    "\n",
    "            data_vecform = data.reshape(-1,)\n",
    "            if normalized:\n",
    "                denom = np.abs(np.max(data_vecform)-np.min(data_vecform)) #np.mean(np.abs(data.reshape(-1,))): main results for paper\n",
    "            else:\n",
    "                denom = 1\n",
    "\n",
    "            return ((np.sum(np.square(err))/t)**0.5)/denom\n",
    "\n",
    "        else:\n",
    "            arr1 = np.sort(np.reshape(data,newshape=(-1,)))[::-1]\n",
    "            arr2 = np.sort(np.reshape(data_replica,newshape=(-1,)))[::-1]\n",
    "            err =  err_func(arr1,arr2)\n",
    "            return (((np.square(err))/t)**0.5)/np.mean(np.abs(data.reshape(-1,)))\n",
    "\n",
    "    elif use_normalizd_form==True:\n",
    "        data_replica = reconstruct_data(data,label, center_id) #data in the input argument is supposed to be yr_normed \n",
    "\n",
    "\n",
    "        for ik in range(kl):\n",
    "            days=np.ix_(label==ik)[0]\n",
    "            if len(days)==1:\n",
    "                err_vec[ik]=0\n",
    "                err_vec_daily[days[0]]=0\n",
    "\n",
    "            else:\n",
    "                if day_wise==False:\n",
    "                    arr1 = np.sort(np.reshape(data[np.ix_(days,)],newshape=(-1,)))[::-1]\n",
    "                    arr2 = np.sort(np.reshape(data_replica[np.ix_(days,)],newshape=(-1,)))[::-1]\n",
    "\n",
    "                    err_vec[ik] = err_func(arr1,arr2)\n",
    "\n",
    "                elif day_wise==True:        \n",
    "                    for d in days:\n",
    "                        arr1 = np.sort(np.reshape(data[d,],newshape=(-1,)))[::-1]\n",
    "                        arr2 = np.sort(np.reshape(data_replica[d,],newshape=(-1,)))[::-1]\n",
    "                        err_vec_daily[d] =  err_func(arr1,arr2)\n",
    "\n",
    "\n",
    "        if day_wise==False:\n",
    "            err=err_vec\n",
    "        elif day_wise==True:\n",
    "            err=err_vec_daily\n",
    "\n",
    "        return (np.sum(np.square(err))/t)**0.5\n",
    "\n",
    "\n",
    "# reconstruct data based on aggregated data\n",
    "def reconstruct_data(data,label_lst, centers_id, center_val=None):\n",
    "    \n",
    "    data_reconstruct=np.zeros_like(data)\n",
    "    for ind in range(len(label_lst)):\n",
    "        if center_val is None:\n",
    "            data_reconstruct[ind,:] = data[centers_id[label_lst[ind]],:]\n",
    "        else:\n",
    "            data_reconstruct[ind,:] = center_val[label_lst[ind],:]\n",
    "    \n",
    "    return data_reconstruct\n",
    "\n",
    "\n",
    "# a dc-to-dc error\n",
    "def err_func(arr_sort_1,arr_sort_2):\n",
    "    return np.linalg.norm(arr_sort_1-arr_sort_2)\n",
    "\n",
    "\n",
    "\n",
    "# find the size of clusters using the labels\n",
    "def clust_size(labels_arr):\n",
    "    klab = len(set(labels_arr))\n",
    "    clust_size_arr = np.empty(klab, dtype='int')\n",
    "    for ik in range(klab):\n",
    "        objs = np.ix_(labels_arr==ik)[0]\n",
    "        clust_size_arr[ik] = len(objs)\n",
    "    \n",
    "    return clust_size_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c569d98",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55bd99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data directory\n",
    "data_dir = \".//2019//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read demand data\n",
    "df_Demand = pd.read_csv(data_dir + 'AB_demand_2019.csv')\n",
    "\n",
    "# convert to numpy array\n",
    "ts_Demand = df_Demand['AB - Alberta Internal Load Hr Avg MW'].to_numpy()\n",
    "\n",
    "###############################\n",
    "\n",
    "# handling missing values can be automated. It is hard-coded here.\n",
    "\n",
    "# fill in missing value(s) by considering nearby values\n",
    "ts_Demand[1634] = (ts_Demand[1633]+ts_Demand[1635])/2 \n",
    "\n",
    "# actual data in 2D\n",
    "ts_Demand_2d = np.reshape(ts_Demand, newshape=(365,24) )\n",
    "\n",
    "# z-norm of actual data and then tranform it to 2D\n",
    "ts_Demand_norm = z_normalize(ts_Demand)\n",
    "ts_Demand_norm_2d = np.reshape(ts_Demand_norm, newshape=(365,24) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9d58752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read wind power data\n",
    "df_WFs = pd.read_csv(data_dir + 'AB_windpower_2019.csv') \n",
    "# df_WFs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef2ecef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read solar data\n",
    "df_solar = pd.read_csv(data_dir + 'AB_solar_2019.csv') # E:\\second_paper\\DATA_To_Study\\2019\n",
    "# df_solar.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6525023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load = pd.read_csv(data_dir + 'AB_demand_2019.csv')\n",
    "ts_load = df_load['AB - Alberta Internal Load Hr Avg MW'].to_numpy()\n",
    "ts_solar = df_solar['MW'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57f1fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_BSR1_Center = df_WFs['AB - Blackspring Ridge Hr Avg MW'].to_numpy()\n",
    "ts_HAL1_CE = df_WFs['AB - Halkirk Wind Power Facility Hr Avg MW'].to_numpy()\n",
    "ts_BTR1_SW = df_WFs['AB - Blue Trail Wind Hr Avg MW'].to_numpy()\n",
    "ts_McBrideLake_SW = df_WFs['AB - McBride Lake Hr Avg MW'].to_numpy()\n",
    "\n",
    "#extra wf data:\n",
    "ts_wf_ext1 = df_WFs['AB - Ardenville Wind Hr Avg MW'].to_numpy()\n",
    "ts_wf_ext2 = df_WFs['AB - Castle Rock Wind Farm Hr Avg MW'].to_numpy()\n",
    "ts_wf_ext3 = df_WFs['AB - Kettles Hill Wind Hr Avg MW'].to_numpy()\n",
    "ts_wf_ext4 = df_WFs['AB - Soderglen Wind Hr Avg MW'].to_numpy()\n",
    "ts_wf_ext5 = df_WFs['AB - Suncor Wintering Hills Hr Avg MW'].to_numpy()\n",
    "ts_wf_ext6 = df_WFs['AB - Enmax Taber Hr Avg MW'].to_numpy()\n",
    "\n",
    "# in the paper, we considered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "038a84cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing value \n",
    "# note: 1634 is the index with missing value \n",
    "# it is hard coded. To automate this, use pd.isnull on the csv \n",
    "\n",
    "ts_BSR1_Center = replace_isolated_missed_value(ts_BSR1_Center, 1634)\n",
    "ts_HAL1_CE = replace_isolated_missed_value(ts_HAL1_CE, 1634)\n",
    "ts_BTR1_SW = replace_isolated_missed_value(ts_BTR1_SW, 1634)\n",
    "ts_McBrideLake_SW = replace_isolated_missed_value(ts_McBrideLake_SW, 1634)\n",
    "\n",
    "ts_load = replace_isolated_missed_value(ts_load, 1634)\n",
    "\n",
    "\n",
    "ts_wf_ext1 = replace_isolated_missed_value(ts_wf_ext1, 1634)\n",
    "ts_wf_ext2 = replace_isolated_missed_value(ts_wf_ext2, 1634)\n",
    "ts_wf_ext3= replace_isolated_missed_value(ts_wf_ext3, 1634)\n",
    "ts_wf_ext4 = replace_isolated_missed_value(ts_wf_ext4, 1634)\n",
    "ts_wf_ext5 = replace_isolated_missed_value(ts_wf_ext5, 1634)\n",
    "ts_wf_ext6 = replace_isolated_missed_value(ts_wf_ext6, 1634)\n",
    "\n",
    "# note: ts_solar has no missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33bbfc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "ts_load = np.copy(ts_load)\n",
    "\n",
    "# solar\n",
    "ts_solar = np.copy(ts_solar)\n",
    "\n",
    "# wind\n",
    "ts_Center = np.copy(ts_BSR1_Center)\n",
    "ts_CE = np.copy(ts_HAL1_CE)\n",
    "\n",
    "ts_SW_1 = np.copy(ts_BTR1_SW)\n",
    "ts_SW_2 = np.copy(ts_McBrideLake_SW)\n",
    "\n",
    "ts_wf_ext1 = np.copy(ts_wf_ext1)\n",
    "ts_wf_ext2 = np.copy(ts_wf_ext2)\n",
    "ts_wf_ext3 = np.copy(ts_wf_ext3)\n",
    "ts_wf_ext4 = np.copy(ts_wf_ext4)\n",
    "ts_wf_ext5 = np.copy(ts_wf_ext5)\n",
    "ts_wf_ext6 = np.copy(ts_wf_ext6)\n",
    "\n",
    "# create dict of data\n",
    "dict_data1d_actual = {'load': ts_load, \n",
    "                      'solar': ts_solar,\n",
    "                      \n",
    "                      'Center':ts_Center, \n",
    "                      'CE': ts_CE, \n",
    "                      'SW_1': ts_SW_1, \n",
    "                      'SW_2': ts_SW_2, \n",
    "                      \n",
    "                      'wf_ext1':ts_wf_ext1,\n",
    "                      'wf_ext2':ts_wf_ext2,\n",
    "                      'wf_ext3':ts_wf_ext3,\n",
    "                      'wf_ext4':ts_wf_ext4,\n",
    "                      'wf_ext5':ts_wf_ext5,\n",
    "                      'wf_ext6':ts_wf_ext6\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8ab132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2D arrays, each row corresponds to one day of year\n",
    "\n",
    "ts_load_minmax = (np.reshape(ts_load, newshape=(365,24))-np.min(ts_load))/(np.max(ts_load)-np.min(ts_load))\n",
    "ts_load_MAXscaled = np.reshape(ts_load, newshape=(365,24))/(np.max(ts_load))\n",
    "\n",
    "ts_solar_minmax = np.reshape(ts_solar, newshape=(365,24))/np.max(ts_solar)\n",
    "\n",
    "ts_Center_minmax = (np.reshape(ts_Center, newshape=(365,24)))/np.max(ts_Center)\n",
    "ts_CE_minmax = (np.reshape(ts_CE,  newshape=(365,24)))/np.max(ts_CE)\n",
    "\n",
    "ts_SW_1_minmax = np.reshape(ts_SW_1, newshape=(365,24))/np.max(ts_SW_1)\n",
    "ts_SW_2_minmax = np.reshape(ts_SW_2, newshape=(365,24))/np.max(ts_SW_2)\n",
    "\n",
    "ts_wf_ext1_minmax = np.reshape(ts_wf_ext1, newshape=(365,24))/np.max(ts_wf_ext1)\n",
    "ts_wf_ext2_minmax = np.reshape(ts_wf_ext2, newshape=(365,24))/np.max(ts_wf_ext2)\n",
    "ts_wf_ext3_minmax = np.reshape(ts_wf_ext3, newshape=(365,24))/np.max(ts_wf_ext3)\n",
    "ts_wf_ext4_minmax = np.reshape(ts_wf_ext4, newshape=(365,24))/np.max(ts_wf_ext4)\n",
    "ts_wf_ext5_minmax = np.reshape(ts_wf_ext5, newshape=(365,24))/np.max(ts_wf_ext5)\n",
    "ts_wf_ext6_minmax = np.reshape(ts_wf_ext6, newshape=(365,24))/np.max(ts_wf_ext6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a76108c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_load_norm = np.reshape(z_normalize(ts_load), newshape=(365,24))\n",
    "ts_solar_norm = np.reshape(z_normalize(ts_solar), newshape=(365,24))\n",
    "\n",
    "\n",
    "# the wind farms ext1 and ext4 will be excluded as they have faulty data.\n",
    "ts_Center_norm = np.reshape(z_normalize(ts_Center), newshape=(365,24))\n",
    "ts_CE_norm = np.reshape(z_normalize(ts_CE),  newshape=(365,24))\n",
    "\n",
    "ts_SW_1_norm = np.reshape(z_normalize(ts_SW_1), newshape=(365,24))\n",
    "ts_SW_2_norm = np.reshape(z_normalize(ts_SW_2), newshape=(365,24))\n",
    "\n",
    "\n",
    "ts_wf_ext1_norm = np.reshape(z_normalize(ts_wf_ext1), newshape=(365,24))\n",
    "ts_wf_ext2_norm = np.reshape(z_normalize(ts_wf_ext2), newshape=(365,24))\n",
    "ts_wf_ext3_norm = np.reshape(z_normalize(ts_wf_ext3), newshape=(365,24))\n",
    "ts_wf_ext4_norm = np.reshape(z_normalize(ts_wf_ext4), newshape=(365,24))\n",
    "ts_wf_ext5_norm = np.reshape(z_normalize(ts_wf_ext5), newshape=(365,24))\n",
    "ts_wf_ext6_norm = np.reshape(z_normalize(ts_wf_ext6), newshape=(365,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c513887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data_norm = {'load':ts_load_norm, \n",
    "                  'solar':ts_solar_norm,\n",
    "\n",
    "                  'Center':ts_Center_norm, \n",
    "                  'CE': ts_CE_norm, \n",
    "                  \n",
    "                  #'SW': ts_SW_norm,\n",
    "                  'SW_1': ts_SW_1_norm,\n",
    "                  'SW_2': ts_SW_2_norm, \n",
    "                  \n",
    "                  'wf_ext1':ts_wf_ext1_norm,\n",
    "                  'wf_ext2':ts_wf_ext2_norm,\n",
    "                  'wf_ext3':ts_wf_ext3_norm,\n",
    "                  'wf_ext4':ts_wf_ext4_norm,\n",
    "                  'wf_ext5':ts_wf_ext5_norm,\n",
    "                  'wf_ext6':ts_wf_ext6_norm\n",
    "                  \n",
    "                  }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict_data_minmax = {'load':ts_load_minmax, \n",
    "                    'solar': ts_solar_minmax,\n",
    "                    \n",
    "                    'Center':ts_Center_minmax, \n",
    "                    'CE': ts_CE_minmax, \n",
    "                    \n",
    "                    #'SW': ts_SW_minmax,\n",
    "                    'SW_1': ts_SW_1_minmax,\n",
    "                    'SW_2': ts_SW_2_minmax,\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    'wf_ext1':ts_wf_ext1_minmax,  \n",
    "                    'wf_ext2':ts_wf_ext2_minmax,\n",
    "                    'wf_ext3':ts_wf_ext3_minmax,\n",
    "                    'wf_ext4':ts_wf_ext4_minmax, \n",
    "                    'wf_ext5':ts_wf_ext5_minmax,\n",
    "                    'wf_ext6':ts_wf_ext6_minmax\n",
    "                      \n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b5def67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2use = dict_data_norm  #dict_data_disc_norm #dict_data_norm\n",
    "dict2use_keys = list(dict2use.keys())\n",
    "\n",
    "distmat_dict={}\n",
    "distmat_lst=[]\n",
    "\n",
    "cntr=0\n",
    "for key in dict2use_keys:\n",
    "    cntr += 1\n",
    "    distmat_dict[key] = pairwise_distances(dict2use[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06796140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove two wfs due to their faulty data\n",
    "keys2ues = dict2use_keys[:]\n",
    "keys2ues.remove('wf_ext1')\n",
    "keys2ues.remove('wf_ext4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8adde38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "576bf6a4",
   "metadata": {},
   "source": [
    "# Grouping resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "81136b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "klst = [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41523528",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 0\n",
    "k_labels = {}\n",
    "\n",
    "for jk, k in enumerate(klst):\n",
    "    k_labels[k] = {}\n",
    "    # print(f\">>> k={k}\")\n",
    "    for  i_key, key in enumerate(keys2ues):\n",
    "        # print(f\"key={key}\")\n",
    "        mydistmat = distmat_dict[key]\n",
    "        km = KMeans(n_clusters=k, init='k-means++', max_iter=100,  n_init=50, random_state = rs).fit(dict2use[key])\n",
    "    \n",
    "        k_labels[k][key] = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf8871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e965a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datasets = len(keys2ues)\n",
    "\n",
    "dict_distmat_between_datasets = {}\n",
    "\n",
    "for jk, k in enumerate(klst):\n",
    "    dict_distmat_between_datasets[k] = np.zeros((n_datasets,n_datasets))\n",
    "    for i in range(n_datasets-1):\n",
    "        ikey = keys2ues[i]\n",
    "        for j in range(i+1,n_datasets):\n",
    "            jkey = keys2ues[j]\n",
    "            I = mutual_info_score(k_labels[k][ikey], k_labels[k][jkey])\n",
    "            d = np.exp(-I)\n",
    "\n",
    "            dict_distmat_between_datasets[k][i,j] = d\n",
    "            dict_distmat_between_datasets[k][j,i] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4946fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage2use=  'average'\n",
    "\n",
    "\n",
    "v = 0 # because  we have just one k here in this case\n",
    "k = klst[v]\n",
    "\n",
    "distmat = dict_distmat_between_datasets[k]\n",
    "n = np.shape(distmat)[0]\n",
    "\n",
    "dists = squareform(distmat)\n",
    "linkage_matrix = linkage(dists, linkage2use)\n",
    "\n",
    "n_feature = np.shape(distmat)[0] # n_feature : number of resources\n",
    "\n",
    "plot_dendogram = False\n",
    "if plot_dendogram:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    dendrogram(linkage_matrix, labels=[str(x) for x in range(0,n_feature)], orientation='top')\n",
    "    plt.title(\"dendrogram\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19a1c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_compactness_list = []\n",
    "maxdist_vec = [] \n",
    "within_sum_lst = []\n",
    "silscore_vec = []\n",
    "\n",
    "for id_f in range(1,n_feature+1):\n",
    "    within_sum = 0\n",
    "    hc = AgglomerativeClustering(id_f, affinity='precomputed', linkage=linkage2use).fit(distmat)\n",
    "    f=hc.labels_\n",
    "    \n",
    "    if ((id_f>1) and (id_f<n_feature)):\n",
    "        silscore = silhouette_score(distmat,f, metric='precomputed')\n",
    "    else:\n",
    "        silscore = 0\n",
    "    silscore_vec.append(silscore)\n",
    "    f_unique = len(set(f))\n",
    "    \n",
    "    max_dist = 0\n",
    "    for f_index in range(f_unique):\n",
    "        idx = np.ix_(f==f_index)[0]\n",
    "        distmat_slice = distmat[np.ix_(idx,idx)]\n",
    "        max_dist = max(max_dist, np.max(distmat_slice))\n",
    "    \n",
    "    maxdist_vec.append(max_dist)\n",
    "\n",
    "\n",
    "\n",
    "fs = 13 # fontsize\n",
    "s = silscore_vec[1:-1]\n",
    "\n",
    "plot_silhoutte = False\n",
    "if plot_silhoutte:\n",
    "    plt.plot(s)\n",
    "    plt.xticks(ticks=np.arange(8), labels=np.arange(2,10),fontsize=fs)\n",
    "    plt.yticks(fontsize=fs)\n",
    "    plt.xlabel('G (Number of Groups)',fontsize=fs)\n",
    "    plt.ylabel('Silhoutte Score',fontsize=fs)\n",
    "    plt.axvline(x=2, ymax = 0.95, linestyle='--', color='r')\n",
    "    plt.scatter(x=2,y=s[2], color='r')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5778750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels we used in the paper\n",
    "keys4plot_paper = ['D', 'SF', 'WF(3)', 'WF(1)', 'WF(5)','WF(6)','WF(7)','WF(8)', 'WF(2)', 'WF(4)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6896253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nclust=4 # number of groups created by grouping data resources (see graph above)\n",
    "\n",
    "distmat = dict_distmat_between_datasets[k]\n",
    "dists = squareform(distmat)\n",
    "\n",
    "linkage_matrix = linkage(dists, linkage2use )\n",
    "\n",
    "f = fcluster(linkage_matrix,nclust,criterion='maxclust')\n",
    "\n",
    "lst = []\n",
    "for id in list(set(f)):\n",
    "    lst.append(np.where(f==id)[0].tolist())\n",
    "\n",
    "# print('grouped data as: ', lst)\n",
    "# print('dict2use_keys: \\n', dict2use_keys)\n",
    "\n",
    "# print('ID of each data set: \\n', [(idx,idx_x) for idx, idx_x in enumerate(keys2ues)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b84293cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_highlevel_features=[]\n",
    "for item in lst:\n",
    "    grouped_data = []\n",
    "    for g in item:\n",
    "        grouped_data.append(keys2ues[g])\n",
    "  \n",
    "    grouped_highlevel_features.append(grouped_data)\n",
    "    \n",
    "# print('grouped_highlevel_features: \\n', grouped_highlevel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b735a624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd7b65df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['load', 'solar'],\n",
       " ['CE', 'wf_ext5'],\n",
       " ['SW_1', 'SW_2', 'wf_ext2', 'wf_ext3'],\n",
       " ['Center', 'wf_ext6']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_highlevel_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f16097d",
   "metadata": {},
   "source": [
    "# proposal M-RD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f10cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OneDay_flag = False\n",
    "only_last_itr = False\n",
    "weighted_last_iteration = False\n",
    "\n",
    "weighted = False\n",
    "\n",
    "n_randomstates = 1000\n",
    "\n",
    "k=20; w=2; \n",
    "# setting w=0 means we are doing ED!\n",
    "# setting w=1 means we are just doing one step head mapping, not too much advantage!\n",
    "# setting w=2, about 10% of length of observation (24) is reasonable [see Eamon Keogh Tutorials]\n",
    "\n",
    "adjustment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b51fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys2remove = ['wf_ext1', 'wf_ext4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f269f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts_load_actual_vals =  np.reshape(ts_load, newshape=(365,24))\n",
    "\n",
    "ts_solar_actual_vals = np.reshape(ts_solar, newshape=(365,24))\n",
    "\n",
    "\n",
    "ts_Center_actual_vals = np.reshape(ts_Center, newshape=(365,24))\n",
    "ts_CE_actual_vals = np.reshape(ts_CE, newshape=(365,24))\n",
    "\n",
    "ts_SW_1_actual_vals = np.reshape(ts_SW_1, newshape=(365,24))\n",
    "ts_SW_2_actual_vals = np.reshape(ts_SW_2, newshape=(365,24))\n",
    "\n",
    "\n",
    "ts_wf_ext1_actual_vals = np.reshape(ts_wf_ext1, newshape=(365,24))\n",
    "ts_wf_ext2_actual_vals = np.reshape(ts_wf_ext2, newshape=(365,24))\n",
    "ts_wf_ext3_actual_vals = np.reshape(ts_wf_ext3, newshape=(365,24))\n",
    "ts_wf_ext4_actual_vals = np.reshape(ts_wf_ext4, newshape=(365,24))\n",
    "ts_wf_ext5_actual_vals = np.reshape(ts_wf_ext5, newshape=(365,24))\n",
    "ts_wf_ext6_actual_vals = np.reshape(ts_wf_ext6, newshape=(365,24))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict_data2d_actual = {'load': ts_load_actual_vals,\n",
    "                      'solar': ts_solar_actual_vals,\n",
    "\n",
    "                      'Center':ts_Center_actual_vals, \n",
    "                      'CE': ts_CE_actual_vals, \n",
    "                      \n",
    "                      'SW_1': ts_SW_1_actual_vals,\n",
    "                      'SW_2': ts_SW_2_actual_vals,\n",
    "\n",
    "                      'wf_ext1':ts_wf_ext1_actual_vals,\n",
    "                      'wf_ext2':ts_wf_ext2_actual_vals,\n",
    "                      'wf_ext3':ts_wf_ext3_actual_vals,\n",
    "                      'wf_ext4':ts_wf_ext4_actual_vals,\n",
    "                      'wf_ext5':ts_wf_ext5_actual_vals,\n",
    "                      'wf_ext6':ts_wf_ext6_actual_vals\n",
    "                      }\n",
    "\n",
    "\n",
    "\n",
    "# print('min per data set')\n",
    "# for key in dict_data2d_actual.keys():\n",
    "#   print(f'at key={key}, minimum is: {np.min(dict_data2d_actual[key])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba336c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'w={w}, and k={k}, OneDay_flag={OneDay_flag}')\n",
    "print('----------------')\n",
    "\n",
    "rs_lst = np.arange(n_randomstates)\n",
    "inertia_lst = np.zeros(len(rs_lst))\n",
    "\n",
    "DTWmat_GroupedFeature = []\n",
    "for rs in rs_lst:\n",
    "    (\n",
    "        _, _, _,_, inertia_tot, _, DTWmat_GroupedFeature, OneDay_Centroid\n",
    "    ) = k_means_multi_centroid_Grouped_HighLevel_Featue_DTWenhanced(\n",
    "        copy.deepcopy(dict_data_norm),  \n",
    "        k=k, \n",
    "        n_itr=50, \n",
    "        rs=rs, \n",
    "        dict_of_data_actual=copy.deepcopy(dict_data2d_actual), \n",
    "        Grouped_Features=grouped_highlevel_features, \n",
    "        keys2remove=keys2remove, \n",
    "        w=w, \n",
    "        DTWmat_GroupedFeature_input = DTWmat_GroupedFeature, \n",
    "        OneDay_flag=OneDay_flag, \n",
    "        weighted=weighted,\n",
    "        only_last_itr=only_last_itr,\n",
    "        weighted_last_iteration = weighted_last_iteration\n",
    "    )\n",
    "    \n",
    "    inertia_lst[rs] = inertia_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.argmin(np.abs(inertia_lst))\n",
    "# print('random state with minimum inertia: ', rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa52d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_data = copy.deepcopy(dict_data_norm) \n",
    "keys2remove = keys2remove\n",
    "\n",
    "for key in keys2remove:\n",
    "    del dict_of_data[key]\n",
    "\n",
    "dict_of_data_keys = list(dict_of_data.keys())\n",
    "\n",
    "dict_pseudokey2actual = {'load':'Load',\n",
    "                         'solar':'Solar',\n",
    "                          \n",
    "                          'Center':'Blackspring', \n",
    "                         'CE': 'Halkirk',\n",
    "                         'SW_1':'Blue_Trail',\n",
    "                         'SW_2':'McBride',\n",
    "                         \n",
    "                         'wf_ext2': 'Castle_Rock_Wind_Farm',\n",
    "                         'wf_ext3':'Kettles_Hill',\n",
    "                         'wf_ext5':'Wintering_Hills',\n",
    "                         'wf_ext6':'Enmax'}\n",
    "\n",
    "dict_of_data_keys_ACTUAL = [dict_pseudokey2actual[x] for x in dict_of_data_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'w={w}, and k={k}, and rs={rs}')\n",
    "# print('grouped_highlevel_features: \\n', grouped_highlevel_features)\n",
    "\n",
    "(\n",
    "    ts_all, \n",
    "    labels_proposal, \n",
    "    center_id_proposal, \n",
    "    dict_of_data_keys, \n",
    "    inertia_tot, \n",
    "    err_dc_dict, \n",
    "    DTWmat_GroupedFeature, \n",
    "    OneDay_Centroid\n",
    ") = k_med_multi_centroid_Grouped_HighLevel_Featue_DTWenhanced(\n",
    "    copy.deepcopy(dict_data_norm), \n",
    "    k=k, \n",
    "    n_itr=25, \n",
    "    rs=rs, \n",
    "    dict_of_data_actual=copy.deepcopy(dict_data2d_actual), \n",
    "    Grouped_Features=grouped_highlevel_features, \n",
    "    keys2remove=keys2remove, \n",
    "    w=w,\n",
    "    DTWmat_GroupedFeature_input = [],\n",
    "    OneDay_flag=OneDay_flag, \n",
    "    weighted = weighted,\n",
    "    only_last_itr=only_last_itr,\n",
    "    weighted_last_iteration=weighted_last_iteration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b5c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OneDay_flag:\n",
    "    center_id_proposal = np.empty_like(center_id_proposal)\n",
    "    for f in range(center_id_proposal.shape[1]):\n",
    "        center_id_proposal[:,f] = OneDay_Centroid\n",
    "\n",
    "df_centers_id = pd.DataFrame(center_id_proposal, columns=dict_of_data_keys_ACTUAL, index=np.arange(k))\n",
    "# save\n",
    "df_centers_id.to_csv('center_id_proposal.csv')\n",
    "\n",
    "\n",
    "df_label=pd.Series(labels_proposal, name='Labels')\n",
    "# save\n",
    "df_label.to_csv('labels_proposal.csv')\n",
    "\n",
    "labels = copy.deepcopy(labels_proposal)\n",
    "center_id = copy.deepcopy(center_id_proposal)\n",
    "\n",
    "for ikey, key in enumerate(dict_of_data_keys):\n",
    "    actual_name = dict_pseudokey2actual[key]\n",
    "    # print(f'key={key} >>> actual_name: {actual_name}')\n",
    "    N, H = np.shape(dict_data_norm[key])\n",
    "\n",
    "    if key=='load':\n",
    "        dat2use = ts_load_MAXscaled\n",
    "    else:\n",
    "        dat2use = dict_data_minmax[key]\n",
    "\n",
    "    data_reconstructed = reconstruct_data(dat2use, labels, center_id[:,ikey], center_val=None)\n",
    "    data_reconstructed_rep = data_reconstructed[center_id[:,ikey],:]\n",
    "  \n",
    "    if adjustment:\n",
    "        data_reconstructed_rep = adjusting_rep (dat2use, data_reconstructed_rep, labels)\n",
    "    \n",
    "    # NOTE: create folder f\"proposal_k{k}\" first! \n",
    "    df = pd.DataFrame(data_reconstructed_rep, columns=np.arange(1,25), index=np.arange(1,k+1))\n",
    "    df.to_csv(f'proposal_k{k}/{actual_name}.csv')\n",
    "  \n",
    "    weights_of_rep = clust_size(labels)\n",
    "    df_weights = pd.Series(weights_of_rep, name='weights_per_cluster')\n",
    "    \n",
    "    # save\n",
    "    df_weights.to_csv('weights_per_cluster.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7056444",
   "metadata": {},
   "source": [
    "# K-Medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77858e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20\n",
    "w=0 \n",
    "\n",
    "#NOTE: w is the window constraint in DTW. Hence, w=0 basically means ED!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14ee3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, H = np.shape(dict_of_data[dict_of_data_keys[0]])\n",
    "# N: number of days in the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6d9865a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_data = copy.deepcopy(dict_data_norm)\n",
    "keys2remove = keys2remove\n",
    "\n",
    "for key in keys2remove:\n",
    "    del dict_of_data[key]\n",
    "\n",
    "dict_of_data_keys = list(dict_of_data.keys())\n",
    "# print('dict_of_data_keys: \\n', dict_of_data_keys)\n",
    "\n",
    "keys2remove = keys2remove\n",
    "\n",
    "\n",
    "dict_pseudokey2actual = {'load':'Load',\n",
    "                         'solar':'Solar',\n",
    "                          \n",
    "                          'Center':'Blackspring', \n",
    "                         'CE': 'Halkirk',\n",
    "                         'SW_1':'Blue_Trail',\n",
    "                         'SW_2':'McBride',\n",
    "                         \n",
    "                         'wf_ext2': 'Castle_Rock_Wind_Farm',\n",
    "                         'wf_ext3':'Kettles_Hill',\n",
    "                         'wf_ext5':'Wintering_Hills',\n",
    "                         'wf_ext6':'Enmax'}\n",
    "\n",
    "\n",
    "dict_of_data_keys_ACTUAL = [dict_pseudokey2actual[x] for x in dict_of_data_keys]\n",
    "\n",
    "# print('dict_pseudokey2actual: \\n', dict_pseudokey2actual)\n",
    "\n",
    "ts_all = np.zeros((N,0))\n",
    "for ikey,key in enumerate(dict_of_data_keys):\n",
    "    ts_all = np.concatenate((ts_all,dict_of_data[key]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f38d4281",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_randomstates=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_lst = np.arange(n_randomstates)\n",
    "inertia_lst = np.zeros(len(rs_lst))\n",
    "\n",
    "distmat_ts_all = pairwise_distances(ts_all)\n",
    "\n",
    "for rs in rs_lst:\n",
    "    kmedoids = KMedoids(n_clusters=k, random_state=rs,  max_iter=50, method='pam', init='random').fit(ts_all) # init='k-medoids++'\n",
    "    inertia_lst[rs] = kmedoids.inertia_\n",
    "    silscore_lst[rs] = silhouette_score(distmat_ts_all, kmedoids.labels_, metric='precomputed')\n",
    "    del kmedoids\n",
    "\n",
    "rs=np.argmin(inertia_lst)\n",
    "rs = rs_candid[np.argmax(silscore_lst_candid)]\n",
    "\n",
    "kmedoids = KMedoids(n_clusters=k, random_state=rs,  max_iter=50, method='pam', init='random').fit(ts_all)\n",
    "kmedoids_lab = kmedoids.labels_\n",
    "kmedoids_centers_id = kmedoids.medoid_indices_\n",
    "\n",
    "keys2remove = keys2remove\n",
    "\n",
    "\n",
    "dict4dc = copy.deepcopy(dict_data2d_actual)\n",
    "for key in keys2remove:\n",
    "    del dict4dc[key]\n",
    "dict4dc_key = list(dict4dc.keys())\n",
    "\n",
    "kmedoids_centers_id_mat=np.empty(shape=(k,len(dict4dc_key)),dtype='int')\n",
    "for i in range(len(dict4dc_key)):\n",
    "    kmedoids_centers_id_mat[:,i]=kmedoids_centers_id\n",
    "\n",
    "\n",
    "err_dc_dict={}\n",
    "for ikey, key in enumerate(dict4dc_key):\n",
    "    data_replica = reconstruct_data(dict4dc[key],kmedoids_lab, kmedoids_centers_id)\n",
    "    err=err_of_dc_grouply(dict4dc[key],data_replica, kmedoids_lab, kmedoids_centers_id, day_wise=False, normalized=True, use_normalizd_form=False)\n",
    "    err_dc_dict[key] = err\n",
    "\n",
    "\n",
    "err_regular = copy.deepcopy(err_dc_dict)\n",
    "df_err=pd.DataFrame([err_regular])\n",
    "df_err.to_csv('err_dc.csv')\n",
    "\n",
    "df_centers_id = pd.DataFrame(kmedoids_centers_id_mat, columns=dict_of_data_keys_ACTUAL, index=np.arange(k))\n",
    "df_centers_id.to_csv('center_id_kmed.csv')\n",
    "\n",
    "df_label=pd.Series(kmedoids_lab, name='Labels')\n",
    "df_label.to_csv('labels_kmed.csv')\n",
    "\n",
    "\n",
    "labels = copy.deepcopy(kmedoids_lab)\n",
    "center_id = copy.deepcopy(kmedoids_centers_id_mat)\n",
    "\n",
    "for ikey, key in enumerate(dict_of_data_keys):\n",
    "    actual_name = dict_pseudokey2actual[key]\n",
    "    \n",
    "    \n",
    "    if key=='load':\n",
    "        dat2use = ts_load_MAXscaled\n",
    "    else:\n",
    "        dat2use = dict_data_minmax[key]\n",
    "\n",
    "\n",
    "\n",
    "    data_reconstructed = reconstruct_data(dat2use, labels, center_id[:,ikey], center_val=None)\n",
    "    data_reconstructed_rep = data_reconstructed[center_id[:,ikey],:]\n",
    "  \n",
    "    df = pd.DataFrame(data_reconstructed_rep, columns=np.arange(1,25), index=np.arange(1,k+1))\n",
    "    \n",
    "    # NOTE: create folder f\"kmed_k{k}\"\n",
    "    df.to_csv(f'kmed_k{k}'+f'/{actual_name}.csv')\n",
    "  \n",
    "  \n",
    "    weights_of_rep = clust_size(labels)\n",
    "    df_weights = pd.Series(weights_of_rep, name='weights_per_cluster')\n",
    "    df_weights.to_csv('weights_per_cluster.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f2a59",
   "metadata": {},
   "source": [
    "# kmean_close2avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "51caf6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20 \n",
    "w=0 \n",
    "# NOTE: w=0 in DTW basically means ED! this is just to be on the safe side and use ED even if we try to use DTW here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4dd32130",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_data = copy.deepcopy(dict_data_norm)\n",
    "keys2remove = keys2remove\n",
    "\n",
    "for key in keys2remove:\n",
    "    del dict_of_data[key]\n",
    "\n",
    "dict_of_data_keys = list(dict_of_data.keys())\n",
    "# print('dict_of_data_keys: \\n', dict_of_data_keys)\n",
    "\n",
    "dict_pseudokey2actual = {'load':'Load',\n",
    "                         'solar':'Solar',\n",
    "                          \n",
    "                          'Center':'Blackspring', \n",
    "                         'CE': 'Halkirk',\n",
    "                         'SW_1':'Blue_Trail',\n",
    "                         'SW_2':'McBride',\n",
    "                         \n",
    "                         'wf_ext2': 'Castle_Rock_Wind_Farm',\n",
    "                         'wf_ext3':'Kettles_Hill',\n",
    "                         'wf_ext5':'Wintering_Hills',\n",
    "                         'wf_ext6':'Enmax'}\n",
    "\n",
    "\n",
    "dict_of_data_keys_ACTUAL = [dict_pseudokey2actual[x] for x in dict_of_data_keys]\n",
    "# print('dict_pseudokey2actual: \\n', dict_pseudokey2actual)\n",
    "\n",
    "ts_all = np.zeros((N,0))\n",
    "for ikey,key in enumerate(dict_of_data_keys):\n",
    "    ts_all = np.concatenate((ts_all,dict_of_data[key]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89f26a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_randomstates=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc187f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('shape of ts_all: \\n', np.shape(ts_all))\n",
    "# print('=====================================')\n",
    "\n",
    "for rs in rs_lst:\n",
    "    km = KMeans(n_clusters=k, random_state=rs).fit(ts_all)\n",
    "    inertia_lst[rs] = km.inertia_\n",
    "\n",
    "rs = np.argmin(inertia_lst)\n",
    "\n",
    "km = KMeans(n_clusters=k, random_state=rs).fit(ts_all)\n",
    "km_lab = km.labels_\n",
    "km_center = np.squeeze(km.cluster_centers_, axis=2)\n",
    "\n",
    "keys2remove = keys2remove\n",
    "dict4dc = copy.deepcopy(dict_data2d_actual)\n",
    "for key in keys2remove:\n",
    "    del dict4dc[key]\n",
    "dict4dc_key = list(dict4dc.keys())\n",
    "\n",
    "\n",
    "km_center_id = np.empty(k, dtype='int')\n",
    "\n",
    "for ik in range(k):\n",
    "    days = np.ix_(km_lab==ik)[0]\n",
    "    days = days.tolist()\n",
    "    ts_slice = ts_all[days,:]\n",
    "\n",
    "    D = pairwise_distances(np.reshape(km_center[ik,:], newshape=(1,-1)), ts_slice)\n",
    "    km_center_id[ik] = days[np.argmin(D[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ad2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys2remove = keys2remove\n",
    "\n",
    "dict4dc = copy.deepcopy(dict_data2d_actual)\n",
    "for key in keys2remove:\n",
    "    del dict4dc[key]\n",
    "\n",
    "dict4dc_key = list(dict4dc.keys())\n",
    "\n",
    "km_centers_id_mat=np.empty(shape=(k,len(dict4dc_key)),dtype='int')\n",
    "for i in range(len(dict4dc_key)):\n",
    "    km_centers_id_mat[:,i]=km_center_id\n",
    "\n",
    "err_dc_dict={}\n",
    "for ikey, key in enumerate(dict4dc_key):\n",
    "    data_replica = reconstruct_data(dict4dc[key],km_lab, km_center_id)\n",
    "    err=err_of_dc_grouply(dict4dc[key],data_replica, km_lab, km_center_id, day_wise=False, normalized=True, use_normalizd_form=False)\n",
    "  \n",
    "    err_dc_dict[key] = err\n",
    "\n",
    "\n",
    "err_regular = copy.deepcopy(err_dc_dict)\n",
    "df_err=pd.DataFrame([err_regular])\n",
    "df_err.to_csv('err_dc.csv')\n",
    "\n",
    "df_centers_id = pd.DataFrame(km_centers_id_mat, columns=dict_of_data_keys_ACTUAL, index=np.arange(k))\n",
    "df_centers_id.to_csv('center_id_km.csv')\n",
    "\n",
    "df_label=pd.Series(km_lab, name='Labels')\n",
    "df_label.to_csv('labels_km.csv')\n",
    "\n",
    "\n",
    "labels = copy.deepcopy(km_lab)\n",
    "center_id = copy.deepcopy(km_centers_id_mat)\n",
    "\n",
    "for ikey, key in enumerate(dict_of_data_keys):\n",
    "    actual_name = dict_pseudokey2actual[key]\n",
    "    N, H = np.shape(dict_data_norm[key])\n",
    "\n",
    "    if key=='load':\n",
    "        dat2use = ts_load_MAXscaled\n",
    "    else:\n",
    "        dat2use = dict_data_minmax[key]\n",
    "\n",
    "    data_reconstructed = reconstruct_data(dat2use, labels, center_id[:,ikey], center_val=None)\n",
    "    data_reconstructed_rep = data_reconstructed[center_id[:,ikey],:]\n",
    "  \n",
    "    df = pd.DataFrame(data_reconstructed_rep, columns=np.arange(1,25), index=np.arange(1,k+1))\n",
    "    \n",
    "    # NOTE: create folder f'km_c2a_k{k}' \n",
    "    df.to_csv(f'km_c2a_k{k}'+f'/{actual_name}.csv')\n",
    "  \n",
    "    weights_of_rep = clust_size(labels)\n",
    "    df_weights = pd.Series(weights_of_rep, name='weights_per_cluster')\n",
    "    df_weights.to_csv('weights_per_cluster.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217123d",
   "metadata": {},
   "source": [
    "# hc_ward_close2avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e95ffbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20\n",
    "w=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_data = copy.deepcopy(dict_data_norm)        #(dict_data_minmax)\n",
    "keys2remove = keys2remove\n",
    "\n",
    "for key in keys2remove:\n",
    "    del dict_of_data[key]\n",
    "\n",
    "dict_of_data_keys = list(dict_of_data.keys())\n",
    "print('dict_of_data_keys: \\n', dict_of_data_keys)\n",
    "\n",
    "\n",
    "#grouped_highlevel_features = grouped_highlevel_features\n",
    "\n",
    "dict_pseudokey2actual = {'load':'Load',\n",
    "                         'solar':'Solar',\n",
    "                          \n",
    "                          'Center':'Blackspring', \n",
    "                         'CE': 'Halkirk',\n",
    "                         'SW_1':'Blue_Trail',\n",
    "                         'SW_2':'McBride',\n",
    "                         \n",
    "                         'wf_ext2': 'Castle_Rock_Wind_Farm',\n",
    "                         'wf_ext3':'Kettles_Hill',\n",
    "                         'wf_ext5':'Wintering_Hills',\n",
    "                         'wf_ext6':'Enmax'}\n",
    "\n",
    "\n",
    "dict_of_data_keys_ACTUAL = [dict_pseudokey2actual[x] for x in dict_of_data_keys]\n",
    "\n",
    "print('dict_pseudokey2actual: \\n', dict_pseudokey2actual)\n",
    "\n",
    "\n",
    "ts_all = np.zeros((N,0))\n",
    "for ikey,key in enumerate(dict_of_data_keys):\n",
    "    ts_all = np.concatenate((ts_all,dict_of_data[key]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e9fd9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = AgglomerativeClustering(n_clusters=k).fit(ts_all)\n",
    "hc_lab = hc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f7d750d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys2remove = keys2remove\n",
    "dict4dc = copy.deepcopy(dict_data2d_actual)\n",
    "for key in keys2remove:\n",
    "    del dict4dc[key]\n",
    "\n",
    "dict4dc_key = list(dict4dc.keys())\n",
    "\n",
    "\n",
    "ntot, htot = np.shape(ts_all)\n",
    "hc_center = np.zeros((k, htot))\n",
    "for ik in range(k):\n",
    "    days = np.ix_(hc_lab==ik)[0]\n",
    "    days = days.tolist()\n",
    "  \n",
    "    sp = ikey * 24\n",
    "    ep = sp + 24\n",
    "\n",
    "    hc_center[ik,sp:ep] = np.mean(ts_all[days,sp:ep],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_center_id = np.empty(k, dtype='int')\n",
    "for ik in range(k):\n",
    "    days = np.ix_(hc_lab==ik)[0]\n",
    "    days = days.tolist()\n",
    "    ts_slice = ts_all[days,:]\n",
    "    \n",
    "    D = pairwise_distances(np.reshape(hc_center[ik,:], newshape=(1,-1)), ts_slice)\n",
    "    hc_center_id[ik] = days[np.argmin(D[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9f98a20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wf_ext1', 'wf_ext4']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys2remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098483a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict4dc = copy.deepcopy(dict_data2d_actual)\n",
    "for key in keys2remove:\n",
    "    del dict4dc[key]\n",
    "\n",
    "dict4dc_key = list(dict4dc.keys())\n",
    "\n",
    "\n",
    "hc_centers_id_mat=np.empty(shape=(k,len(dict4dc_key)),dtype='int')\n",
    "for i in range(len(dict4dc_key)):\n",
    "    hc_centers_id_mat[:,i]=hc_center_id\n",
    "\n",
    "\n",
    "err_dc_dict={}\n",
    "for ikey, key in enumerate(dict4dc_key):\n",
    "    data_replica = reconstruct_data(dict4dc[key],hc_lab, hc_center_id)\n",
    "    err=err_of_dc_grouply(dict4dc[key],data_replica, hc_lab, hc_center_id, day_wise=False, normalized=True, use_normalizd_form=False)\n",
    "    err_dc_dict[key] = err\n",
    "\n",
    "err_regular = copy.deepcopy(err_dc_dict)\n",
    "df_err=pd.DataFrame([err_regular])\n",
    "df_err.to_csv('err_dc.csv')\n",
    "\n",
    "df_centers_id = pd.DataFrame(hc_centers_id_mat, columns=dict_of_data_keys_ACTUAL, index=np.arange(k))\n",
    "df_centers_id.to_csv('center_id_hc.csv')\n",
    "\n",
    "df_label=pd.Series(hc_lab, name='Labels')\n",
    "df_label.to_csv('labels_hc.csv')\n",
    "\n",
    "\n",
    "labels = copy.deepcopy(hc_lab)\n",
    "center_id = copy.deepcopy(hc_centers_id_mat)\n",
    "\n",
    "for ikey, key in enumerate(dict_of_data_keys):\n",
    "    actual_name = dict_pseudokey2actual[key]\n",
    "    N, H = np.shape(dict_data_norm[key])\n",
    "\n",
    "    if key=='load':\n",
    "        dat2use = ts_load_MAXscaled\n",
    "    else:\n",
    "        dat2use = dict_data_minmax[key]\n",
    "\n",
    "\n",
    "\n",
    "    data_reconstructed = reconstruct_data(dat2use, labels, center_id[:,ikey], center_val=None)\n",
    "    data_reconstructed_rep = data_reconstructed[center_id[:,ikey],:]\n",
    "  \n",
    "    df = pd.DataFrame(data_reconstructed_rep, columns=np.arange(1,25), index=np.arange(1,k+1))\n",
    "    \n",
    "    # NOTE: create folder f'hc_c2a_k{k}'\n",
    "    df.to_csv(f'hc_c2a_k{k}'+f'/{actual_name}.csv')\n",
    "  \n",
    "    weights_of_rep = clust_size(labels)\n",
    "    df_weights = pd.Series(weights_of_rep, name='weights_per_cluster')\n",
    "    df_weights.to_csv('weights_per_cluster.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a306f3",
   "metadata": {},
   "source": [
    "# KmHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "868942fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20\n",
    "w=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "68957cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMHC(ts_data,kmean_labels,k_main,distmat_of_data, w=24):\n",
    "    # note: kmean_labels is a set of labels obtained from clustering ts_data into\n",
    "    # k1 clusters \n",
    "    # k_main is the total number of clusters. Hence, k_main//k1 is the number of sub-clusters in the \n",
    "    # second step of KmHC, where hierarchical clustering with min-max linkage is used.\n",
    "    center_id_main = np.empty(k_main, dtype='int')\n",
    "    labels_main = np.empty(len(kmean_labels), dtype='int')\n",
    "    \n",
    "    k_kmean = len(set(kmean_labels))\n",
    "    k_per_group = k_main // k_kmean\n",
    "  \n",
    "    for ik in range(k_kmean):\n",
    "        days = np.ix_(kmean_labels==ik)[0]\n",
    "        distmat_of_data_sliced = distmat_of_data[np.ix_(days,days)]\n",
    "        data_sliced = ts_data[days]\n",
    "\n",
    "        hc_lab_mat = HC_MinMax_klst(data_sliced, np.array([k_per_group]), \n",
    "                                    w=w, \n",
    "                                    flag_distmat=True,  \n",
    "                                    given_distmat=distmat_of_data_sliced)\n",
    "\n",
    "        hc_lab = hc_lab_mat[0,:]\n",
    "\n",
    "        center_id_tmp = minmax_center_index_finder(hc_lab,distmat_of_data_sliced)\n",
    "\n",
    "        for jk in range(k_per_group):\n",
    "            actual_label = int(ik*k_per_group + jk)\n",
    "            center_id_main[actual_label] = int(days[int(center_id_tmp[jk])])\n",
    "            obj_idx = np.ix_(hc_lab==jk)[0]\n",
    "            for idx in obj_idx:\n",
    "                labels_main[int(days[idx])] = actual_label\n",
    "\n",
    "    return (labels_main, center_id_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c7488b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_data = copy.deepcopy(dict_data_norm)  \n",
    "keys2remove = keys2remove\n",
    "\n",
    "for key in keys2remove:\n",
    "    del dict_of_data[key]\n",
    "\n",
    "dict_of_data_keys = list(dict_of_data.keys())\n",
    "\n",
    "\n",
    "\n",
    "dict_pseudokey2actual = {'load':'Load',\n",
    "                         'solar':'Solar',\n",
    "                          \n",
    "                          'Center':'Blackspring', \n",
    "                         'CE': 'Halkirk',\n",
    "                         'SW_1':'Blue_Trail',\n",
    "                         'SW_2':'McBride',\n",
    "                         \n",
    "                         'wf_ext2': 'Castle_Rock_Wind_Farm',\n",
    "                         'wf_ext3':'Kettles_Hill',\n",
    "                         'wf_ext5':'Wintering_Hills',\n",
    "                         'wf_ext6':'Enmax'}\n",
    "\n",
    "\n",
    "dict_of_data_keys_ACTUAL = [dict_pseudokey2actual[x] for x in dict_of_data_keys]\n",
    "\n",
    "ts_all = np.zeros((N,0))\n",
    "for ikey,key in enumerate(dict_of_data_keys):\n",
    "    ts_all = np.concatenate((ts_all,dict_of_data[key]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_data_keys = list(dict_of_data.keys())\n",
    "print('dict_of_data_keys: \\n', dict_of_data_keys)\n",
    "\n",
    "nkey = len(dict_of_data_keys)\n",
    "\n",
    "dict_of_data_keys_idx = {}\n",
    "for idx_mykey, my_key in enumerate(dict_of_data_keys):\n",
    "    dict_of_data_keys_idx[my_key] = idx_mykey\n",
    "\n",
    "N, H = np.shape(dict_of_data[dict_of_data_keys[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "996a748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_all_original = np.zeros((N,0))\n",
    "ts_all_original_modified = np.zeros((N,0))\n",
    "\n",
    "huge_val_seperator = np.ones((N,1))*(1e6)\n",
    "\n",
    "for ikey,key in enumerate(dict_of_data_keys):\n",
    "    ts_all_original = np.concatenate((ts_all_original,dict_of_data[key]),axis=1)\n",
    "    ts_all_original_modified = np.concatenate((ts_all_original_modified, huge_val_seperator, dict_of_data[key]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "54a14359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that adding huge_val_seperator is necessary which was missed in KmHC. Otherwise, one can map a sequence of a data set\n",
    "# onto a sequence of another data set! However, we cannot do that as they have different nature.\n",
    "dtwmat_modified_w24 = cdist_dtw(ts_all_original_modified, global_constraint=\"sakoe_chiba\", sakoe_chiba_radius=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# km_lab: labels of data being clustered into k1 groups \n",
    "# NOTE: In the first stage of KmHC (as what presented in the original paper), one needs to\n",
    "# cluster data into k1 groups. So, for 20 clusters, we cluster them first to k1 groups, and then, we apply hc(minmax)\n",
    "# in each group\n",
    "\n",
    "# k1=10 --> use kmeans to find km_lab\n",
    "\n",
    "km = KMeans(n_clusters=10, random_state=0).fit(ts_all) # alternatively, it is better to run for 1000 rs, and choose the best one\n",
    "# i.e. the one with lowest inertia\n",
    "\n",
    "km_lab = km.labels_\n",
    "KMHC_labels, KMHC_centroids_ID = KMHC(ts_all_original_modified,km_lab,k,dtwmat_modified_w24, w=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19422a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_id_mat = np.empty((k,len(dict_of_data_keys)), dtype='int')\n",
    "for i in range(len(dict_of_data_keys)):\n",
    "    centroid_id_mat[:,i]=KMHC_centroids_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_dc_dict={}\n",
    "for ikey, key in enumerate(dict_of_data_keys):\n",
    "    data_replica = reconstruct_data(dict_of_data[key],KMHC_labels, KMHC_centroids_ID)\n",
    "    err=err_of_dc_grouply(dict_of_data[key],data_replica, KMHC_labels, KMHC_centroids_ID, day_wise=False, normalized=True, use_normalizd_form=False)\n",
    "  \n",
    "    err_dc_dict[key] = err\n",
    "\n",
    "\n",
    "err = copy.deepcopy(err_dc_dict)\n",
    "df_err=pd.DataFrame([err])\n",
    "df_err.to_csv('err_dc.csv')\n",
    "\n",
    "df_label=pd.Series(KMHC_labels, name='Labels')\n",
    "df_label.to_csv('labels_KMHC.csv')\n",
    "\n",
    "df_centers_id = pd.DataFrame(centroid_id_mat, columns=dict_of_data_keys_ACTUAL, index=np.arange(k))\n",
    "df_centers_id.to_csv('center_id_KMHC.csv')\n",
    "\n",
    "labels = copy.deepcopy(KMHC_labels)\n",
    "center_id = copy.deepcopy(centroid_id_mat)\n",
    "\n",
    "for ikey, key in enumerate(dict_of_data_keys):\n",
    "    actual_name = dict_pseudokey2actual[key]\n",
    "    N, H = np.shape(dict_data_norm[key])\n",
    "\n",
    "    if key=='load':\n",
    "        dat2use = ts_load_MAXscaled\n",
    "    else:\n",
    "        dat2use = dict_data_minmax[key]\n",
    "  \n",
    "  \n",
    "  \n",
    "    data_reconstructed = reconstruct_data(dat2use, labels, center_id[:,ikey], center_val=None)\n",
    "    data_reconstructed_rep = data_reconstructed[center_id[:,ikey],:]\n",
    "  \n",
    "    df = pd.DataFrame(data_reconstructed_rep, columns=np.arange(1,25), index=np.arange(1,k+1))\n",
    "    df.to_csv(f'KMHC_k{k}'+f'/{actual_name}.csv') # create the dir first\n",
    "  \n",
    "  \n",
    "    weights_of_rep = clust_size(labels)\n",
    "    df_weights = pd.Series(weights_of_rep, name='weights_per_cluster')\n",
    "    df_weights.to_csv('weights_per_cluster.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
